{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self, approximate:str = 'none')->None: \n",
    "        super().__init__()\n",
    "        self.approximate = approximate \n",
    "    \n",
    "    def forward(self, input:Tensor)->Tensor:\n",
    "        return 0.5 * input * (1 + torch.tanh(math.sqrt(math.pi / 2) * (input + 0.044715 * input ** 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig: \n",
    "    attn_dropout = 0.1 \n",
    "    embed_dropout = 0.1 \n",
    "    ff_dropout = 0.1 \n",
    "\n",
    "    def __init__(self, vocab_size:int, max_len:int, **kwargs):\n",
    "        \"\"\"\n",
    "        Configuration Base Class for GPT\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int):Size of the vocabulary; how many tokens the model is expected to know \n",
    "            max_len (int): refers to maximum length that can be processed by the model\n",
    "        \n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size \n",
    "        self.max_len = max_len \n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "    \n",
    "class GPT1Config(GPTConfig):\n",
    "    num_heads = 12 \n",
    "    num_blocks = 12 \n",
    "    embed_dim = 768 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim \n",
    "        self.num_heads = config.num_heads \n",
    "        assert embed_dim % self.num_heads == 0, \"Invalid heads and embedding dimensions\"\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(config.ff_dropout)\n",
    "        # https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723\n",
    "        # In the decoder we need a mask layer, but we don't want that to be trained. So, it is used as self.register buffer\n",
    "        # See torch.tril for more details\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(config.max_len, config.max_len)).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0,2,3,1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1,2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1,2)\n",
    "\n",
    "        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n",
    "        mask = self.mask[:,:,:seq_len,:seq_len]\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = self.attn_dropout(attn)\n",
    "        attn = F.softmax(attn, dim = -1)\n",
    "        y = torch.matmul(attn, v)\n",
    "        y = y.transpose(1,2)\n",
    "        y = y.transpose(1, 2)\n",
    "        y = y.reshape(batch_size, seq_len, -1)\n",
    "        y = self.proj_dropout(self.proj(y))\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            GELU(), \n",
    "            nn.Linear(embed_dim * 4, embed_dim), \n",
    "            nn.Dropout(config.ff_dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x+ self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.max_len = config.max_len\n",
    "        self.tok_embed = nn.Embedding(\n",
    "            config.vocab_size, embed_dim\n",
    "        )\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, config.max_len, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.embed_dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.num_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, x, target = None):\n",
    "        seq_len = x.size(1)\n",
    "        assert seq_len <= self.max_len, \"Sequence longer than model's maximum length \"\n",
    "\n",
    "        tok_embedding = self.tok_embed(x)\n",
    "        pos_embedding = self.pos_embed[:,:seq_len, :]\n",
    "\n",
    "        x = self.dropout(tok_embedding + pos_embedding)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10 \n",
    "max_len = 12 \n",
    "\n",
    "config = GPT1Config(vocab_size, max_len) \n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence longer than model's maximum length \n"
     ]
    }
   ],
   "source": [
    "# Creating a Dummy input and testing \n",
    "seq_len = 15 \n",
    "batch_size = 8 \n",
    "test_input = torch.randint(high = vocab_size, size = (batch_size, seq_len))\n",
    "\n",
    "try:\n",
    "    model(test_input).shape \n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_input[:,:max_len]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testMoses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
